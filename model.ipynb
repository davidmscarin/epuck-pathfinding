{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def compute_reward(self, robot_position, target_position, collision_status):\n",
    "    reward = 0\n",
    "    current_distance = np.linalg.norm(target_position - robot_position)\n",
    "\n",
    "    # Reward for moving closer to the target\n",
    "    if self.previous_distance > current_distance:\n",
    "        reward += (self.previous_distance - current_distance) * 10\n",
    "    else:\n",
    "        reward -= 1  # Penalty for moving away from the target\n",
    "\n",
    "    # Check for collision and apply a large penalty if a collision occurs\n",
    "    if collision_status:\n",
    "        reward -= 100  \n",
    "\n",
    "    # Reward for reaching the target within a predefined threshold\n",
    "    if current_distance < self.target_threshold:\n",
    "        reward += 100  \n",
    "\n",
    "    self.previous_distance = current_distance  # Update previous distance for next reward calculation\n",
    "    return reward\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size=2, hidden_size=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden layer\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Hidden to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Input passes through the first fully connected layer\n",
    "        x = self.relu(x)  # Activation function is applied\n",
    "        x = self.fc2(x)   # Result passes through the output layer\n",
    "        return x\n",
    "\n",
    "# Assuming the use of this network in a training loop\n",
    "def train(env, model, episodes, learning_rate=0.001, gamma=0.99):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            velocities = model(state_tensor)\n",
    "\n",
    "            # Select action based on the model output\n",
    "            linear_velocity = velocities[0].item()\n",
    "            angular_velocity = velocities[1].item()\n",
    "            action = (linear_velocity, angular_velocity)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example to instantiate and use the model\n",
    "input_size = 360  # Example for 360-degree LIDAR data points\n",
    "hidden_size = 128  # Hidden layer size\n",
    "model = QNetwork(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
