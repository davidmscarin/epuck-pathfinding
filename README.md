training a agent to learn to navigate an unknown environment in order to find the optimal path from a random origin position to a known target while avoiding obstacles. The agent is a Webots ”e-puck” robot and the training and testing were simulated in a custom Webots world. The training process uses the robot’s Light Detection and Ranging sensor (LiDAR), which provides
point cloud readings as input to different neural networks; we experimented with a Deep-Q Learning (DQN) approach using a manually discretized action space where a policy network predicts the next optimal action from among a pre-specified set of actions, as well as with a Proximal Policy Optimization (PPO) approach, where the policy network attempts to return two continous values: the optimal linear and angular velocity for the robot’s actuators. The reward function remained the same in both cases, being based on the known distance of the robot from the target as well as detecting collisions and instances where the target was reached.
